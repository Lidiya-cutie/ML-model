{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Мультиклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что делать, когда, например, классификация автомобилей по различным маркам или определение национальности по фотографии и т.д?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таком случае используется очень простой подход, который называется «один против всех» (one-vs-over)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея этого подхода очень простая. Если у нас есть $k$ различных классов ($k>2$), давайте обучим $k$ классификаторов, каждый из которых будет предсказывать вероятности принадлежности каждого объекта к определённому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, у нас есть три класса, обозначенные как 0, 1 и 2. Тогда мы обучаем три классификатора: первый из них учится отличать класс 0 от классов 1 и 2, второй — класс 1 от классов 0 и 2, а третий — класс 2 от классов 1 и 0. Таким образом, класс, на который «заточен» классификатор, мы обозначаем как 1, а остальные классы — как 0.\n",
    "\n",
    "Когда каждая из трёх моделей сделает предсказание вероятностей для объекта, итоговый классификатор будет выдавать класс, который соответствует самой «уверенной» модели.\n",
    "\n",
    "Схематично это можно представить следующим образом:\n",
    "\n",
    "1[](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e564fbde1a630f729c88ec22ef7134b9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/ML_3_4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы используем в качестве классификатора логистическую регрессию и количество факторов равно двум ($x_1$ и $x_2$), то можно изобразить тепловую карту вероятностей принадлежности к каждому из классов в каждой точке пространства, а также разделяющие плоскости, которые образуются при пороге вероятности в 0.5. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f5cd06961e15e69623e981930d396ef1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-3_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На тепловых картах каждый класс обозначен своим цветом: 0 — зелёным, 1 — жёлтым, 2 — синим. Чем ярче цвет, тем выше вероятность принадлежности к каждому к классу в этой области пространства.\n",
    "\n",
    "В результате у нас получится три различных пространства вероятностей, что-то вроде трёх параллельных реальностей. Чтобы собрать всё это воедино, мы выбираем в каждой точке пространства максимум из вероятностей. Получим следующую картину:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d8cd72dcd4419c4c14224a829192955c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-3_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель логистической регрессии легко обобщается на случай мультиклассовой классификации. Пусть мы построили несколько разделяющих плоскостей с различными наборами параметров $k$, где $k$ — номер классификатора. То есть имеем $K$ разделяющих плоскостей:\n",
    "\n",
    "$z_k = w_{0k} + \\sum \\limits^m_{j=1}w_{jk}x_k = w_k \\ \\cdot \\ x$\n",
    "\n",
    "Чтобы преобразовать результат каждой из построенных моделей в вероятности в логистической регрессии, используется функция softmax — многомерный аналог сигмоиды:\n",
    "\n",
    "$\\hat P_k = softmax(z_k) = {exp(\\hat y_k) \\over {\\sum^K_{k=1}exp(\\hat y_{jk})}}$\n",
    "\n",
    "Данная функция выдаёт нормированные вероятности, то есть в сумме для всех классов вероятность будет равна 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> МУЛЬТИКЛАССОВАЯ КЛАССИФИКАЦИЯ НА PYTHON\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
