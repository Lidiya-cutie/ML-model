{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kлассификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача классификации (classification) — задача, в которой мы пытаемся предсказать класс объекта на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f1c52bc11a0e81a47e3096dfb2750fdc/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml1-3_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда классов, которые мы хотим предсказать, только два, классификация называется **бинарной**. Например, мы можем предсказать, болен ли пациент раком, является ли изображение человеческим лицом, является ли письмо спамом и т. д.\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, более двух, классификация называется **мультиклассовой (многоклассовой)**. Например, предсказание модели самолёта по радиолокационным снимкам, классификация животных на фотографиях, определение языка, на котором говорит пользователь, разделение писем на группы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/295dc860fdc114b9449701a16f7b6200/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые решают задачу классификации, называются **классификаторами (classifier)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если взять в качестве разделяющей поверхности некоторую плоскость (ровная поверхность на первом рисунке), то мы получаем модель логистической регрессии, которая тесно связана с рассмотренной нами ранее линейной регрессией.\n",
    "\n",
    "Давайте для начала вспомним, как выглядит уравнение модели линейной регрессии в общем случае:\n",
    "\n",
    "$\\hat y = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m = w_0 + \\sum \\limits^{m}_{j=1}w_jx_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае это уравнение гиперплоскости, которая стремится приблизить зависимость целевой переменной от $m$ факторов.\n",
    "\n",
    "* Когда фактор всего один, уравнение задаёт прямую:\n",
    "$\\hat y = w_0 +w_1x$\n",
    "\n",
    "* Когда факторов два, уравнение задаёт плоскость:\n",
    "$\\hat y = w_0 +w_1x_1 + w_2x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но всё это работает только в том случае, когда целевой признак $y$, который мы хотим предсказать, является числовым, например цена, вес, время аренды и т. д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Логистическая регрессия (Logistic Regression)** — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция **(logistic function)** $\\sigma (z)$ — отсюда и название модели. Однако более распространённое название этой функции — **сигмόида (sigmoid)**. Записывается она следующим образом:\n",
    "\n",
    "$\\sigma (z) = {1 \\over {1 + e^{-z}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе логистической регрессии лежит логистическая функция **(logistic function)** $\\sigma$ — отсюда и название модели. Однако более распространённое название этой функции — сигмόида (sigmoid). Записывается она следующим образом:\n",
    "\n",
    "$\\sigma (z) = {1 \\over {1 + e^{-z}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А вот график зависимости сигмоиды от аргумента $z$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/17e5df54243ab349c94672502f9f4dd0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чём преимущество этой функции?\n",
    "\n",
    "У сигмоиды есть два очень важных для нас свойства:\n",
    "\n",
    "* Значения сигмоиды $\\sigma (z)$ лежат в диапазоне от 0 до 1 при любых значения аргумента $z$: какой бы $z$ вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "* Сигмоида выдаёт значения $\\sigma (z) > 0.5$ при её аргументе $z > 0$, $\\sigma (z) < 0.5$ — при $z < 0$ и $\\sigma (z) = 0.5$ — при $z = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для бинарной классификации описанное выше будет выглядеть следующим образом:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/8e663f443f23b393252ae85246b5d23f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как это работает, на примере.\n",
    "\n",
    "Пусть мы каким-то образом обучили модель линейной регрессии предсказывать положительные числа для спам-писем и отрицательные — для обычных писем.\n",
    "\n",
    "Подаём характеристики письма $x_1, x_2, ..., x_m$ в выражение для линейной регрессии и получаем ответ модели, например $z = 1.5$. Тогда, подставив его в сигмоиду, получим:\n",
    "\n",
    "$\\hat P = \\sigma (z) = {1 \\over {1 + e^{-z}}} = {1 \\over {1 + e^{-1.5}}} = 0.82$\n",
    "\n",
    "Таким образом, вероятность того, что данный объект принадлежит классу спама, равна 0.82, что больше порогового значения 0.5. То есть мы относим данное письмо к спаму: $\\hat y = 1$.\n",
    "\n",
    "Пусть теперь мы подали на вход модели характеристики другого письма и получили $z = -0.91$. Тогда, подставив этот результат в сигмоиду, получим:\n",
    "\n",
    "$\\hat P = \\sigma (z) = {1 \\over {1 + e^{-z}}} = {1 \\over {1 + e^{0.91}}} = 0.28$\n",
    "\n",
    "Вероятность того, что данный объект принадлежит классу спама, равна 0.28, что меньше порогового значения 0.5. Мы относим данное письмо к обычным письмам: $\\hat = 0$.\n",
    "\n",
    "Кстати, вероятность того, что это письмо будет обычным, равна противоположной вероятности: $Q = 1 - 0.28 = 0.72$. \n",
    "\n",
    "Полученное выражение для оценки вероятности $\\hat P$ и будет называться моделью логистической регрессии:\n",
    "\n",
    "$\\hat P = {1 \\over {1 + e^{-w_0 - \\sum^m_{j=1}w_jx_j}}}$\n",
    "\n",
    "$\\hat y = I[\\hat P]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример.\n",
    "\n",
    "Мы пытаемся предсказать поступление студента в аспирантуру в зависимости от результатов двух экзаменов. Целевой признак $y$ — результат поступления (admission outcome) с двумя возможными значениями: поступил или не поступил. Факторы: $x_1$ — результат сдачи первого экзамена (Exam1 Score) и $x_2$ — результат сдачи второго (Exam 2 Score). Будем предсказывать вероятность поступления с помощью логистической регрессии.\n",
    "\n",
    "Изобразим зависимость в пространстве двух факторов (вид сверху) в виде диаграммы рассеяния, а целевой признак отобразим в виде точек (непоступившие) и крестиков (поступившие).\n",
    "\n",
    "Если рассматривать уравнение линейной регрессии отдельно от сигмоиды, то геометрически построить логистическую регрессию на основе двух факторов — значит найти такие коэффициенты $w_0$, $w_1$ и $w_2$ уравнения плоскости, при которых наблюдается наилучшее разделение пространства на две части.\n",
    "\n",
    "$z = w_0 + w_1x_1 + w_2x_2$\n",
    "\n",
    "Тогда выражение для $z$ будет задавать в таком пространстве плоскость (в проекции вида сверху — прямую), которая разделяет всё пространство на две части. Над прямой вероятность поступления будет $> 0.5$, а под прямой $< 0.5$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ac31a24479b4371a8228994befb080a1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициенты построенной выше плоскости равны (как их найти, обсудим позже):\n",
    "\n",
    "$w_0 = -25.05$\n",
    "\n",
    "$w_1 = 0.205$\n",
    "\n",
    "$w_2 = 0.2$\n",
    "\n",
    "Тогда модель логистической регрессии будет иметь вид:\n",
    "\n",
    "$z = -25.05 + 0.205x_1 + 0.2x_2$\n",
    "\n",
    "$\\hat P = \\sigma (z) = {1 \\over {1 + e^{-z}}}$\n",
    "\n",
    "Появляется новый абитуриент, и мы хотим предсказать вероятность его поступления. Баллы студента: $x_1 = 67$, $x_2 = 53$. Заметьте, что точка с такими координатами находится ниже нашей плоскости (то есть абитуриент, скорее всего, не поступит).\n",
    "\n",
    "Тогда: \n",
    "\n",
    "$z = -25.05 + 0.205 \\cdot 67 + 0.2 \\cdot 53 = -0.71$\n",
    "\n",
    "$\\hat P = \\sigma (z) = {1 \\over {1 + e^{0.71}}} = 0.32$\n",
    "\n",
    "Итак, оценка вероятности поступления студента составляет 0.32, то есть его шанс поступления составляет 32 %.\n",
    "\n",
    "А что если мы возьмём точку, лежащую выше прямой?\n",
    "\n",
    "Например, появился абитуриент с баллами $x_1 = 80$, $x_2 = 75$. Подставим его баллы в нашу модель логистической регрессии, чтобы понять, какова оценочная вероятность поступления:\n",
    "\n",
    "$z = -25.05 + 0.205 \\cdot 80 + 0.2 \\cdot 75 = 6.34$\n",
    "\n",
    "$\\hat P = \\sigma (z) = {1 \\over {1 + e^{-6.34}}} = 0.99$\n",
    "\n",
    "Таким образом, оценка вероятности поступления абитуриента составляет 0.99, шанс поступления — 99 %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В чём математический секрет?\n",
    "\n",
    "Математически подстановка в уравнение плоскости точки, которая не принадлежит ей (находится ниже или выше), означает вычисление расстояния от этой точки до плоскости.\n",
    "\n",
    "* Если точка находится ниже плоскости, расстояние будет отрицательным ($z < 0$).\n",
    "* Если точка находится выше плоскости, расстояние будет положительным ($z > 0$).\n",
    "* Если точка находится на самой плоскости, $z = 0$.\n",
    "\n",
    "Мы знаем, что подстановка отрицательных чисел в сигмоиду приведёт к вероятности \\hat P < 0.5$, а постановка положительных — к вероятности $\\hat P > 0.5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, ключевым моментом в предсказании логистической регрессии является расстояние от точки до разделяющей плоскости в пространстве факторов. Это расстояние в литературе часто называется **отступом (margin)**. \n",
    "\n",
    "Чем больше расстояние от точки, находящейся выше разделяющей плоскости, до самой плоскости, тем больше оценка вероятности принадлежности к классу 1.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/30baeed1eeb22b64aa28d75952115e87/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для случая зависимости целевого признака $y$ от трёх факторов $x_1$, $x_2$ и $x_3$, например от баллов за два экзамена и рейтинга университета, из которого выпустился абитуриент, выражение для  будет иметь вид:\n",
    "\n",
    "$z = w_0 + w_1x_1 + w_2x_2 + w_3x_3$\n",
    "\n",
    "Уравнение задаёт плоскость в четырёхмерном пространстве. Но если вспомнить, что $y$ — категориальный признак и классы можно обозначить цветом, то получится перейти в трёхмерное пространство. Разделяющая плоскость будет выглядеть следующим образом:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ecda661b626060bcc2c4dc6593249013/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае, когда у нас есть зависимость от $m$ факторов, линейное выражение, находящееся под сигмоидой, будет обозначать **разделяющую гиперплоскость**.\n",
    "\n",
    "$z = w_0 + w_1x_1 + w_2x_2 + ... + w_mx_m = w_0 + \\sum \\limits^m_{j=1}w_jx_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kак найти такие коэффициенты $w = (w_0, w_1, w_2, ..., w_m)$, чтобы гиперплоскость разделяла пространство наилучшим образом?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/5fac5fe11d423f674949523e3db643c9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml1-2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь нужен подход - **метод максимального правдоподобия (Maximum Likelihood Estimation — MLE)**. \n",
    "\n",
    "**Правдоподобие** — это оценка того, насколько вероятно получить истинное значение целевой переменной $y$ при данных $x$ и параметрах $w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель метода — найти такие параметры $w = (w_0, w_1, w_2, ..., w_m)$, в которых наблюдается максимум функции правдоподобия. Подробнее о выводе формулы вы можете прочитать [здесь](https://habr.com/ru/post/485872/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведём только конечную формулу:\n",
    "\n",
    "$lieklihood = \\sum \\limits_i^n(y_ilog(\\hat P_i) + (1 - y_i)log(1 - \\hat P_i)) \\rightarrow \\max_w$\n",
    "\n",
    "Давайте разберёмся, что есть что и как работает эта функция.\n",
    "\n",
    "* $n$ — количество наблюдений.\n",
    "* $y_i$ — это истинный класс (1 или 0) для $i$-ого объекта из набора данных.\n",
    "* $\\hat P_i = \\sigma (z_i)$ — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора данных.\n",
    "* $z_i$ — результат подстановки $i$-ого объекта из набора данных в уравнение разделяющей плоскости $z_i = \\bar w \\cdot \\bar x_i$.\n",
    "*$log$ — логарифм (обычно используется натуральный логарифм по основанию $e - ln$)."
   ]
  },
  {
   "attachments": {
    "2023-08-12_13-30-45.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример расчёта функции правдоподобия\n",
    "\n",
    "Вернёмся к примеру с абитуриентами. Пусть у нас есть выборка из четырёх студентов с оценками по двум экзаменам: $x_1$ и $x_2$. Возьмём уравнение разделяющей плоскости, которое мы использовали ранее:\n",
    "\n",
    "$z = -25.05 + 0.205x_1 + 0.2x_2$\n",
    "\n",
    "Мы взяли всех студентов из выборки в формулу сигмоиды и получили оценочную вероятность поступления каждого из студентов:\n",
    "\n",
    "![2023-08-12_13-30-45.png](attachment:2023-08-12_13-30-45.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем, чему равна функция правдоподобия при данных предсказаниях вероятностей:\n",
    "\n",
    "$lieklihood = \\sum \\limits_i^n(y_ilog(\\hat P_i) + (1 - y_i)log(1 - \\hat P)) = ((0 log (0.2) + (1 - 0) log (1 - 0.2)) + (0 log (0.8) + (1 - 0) log (1 - 0.8)) + (1 log (1) + (1 - 1) log (1 -1)) + (1 log (0.6) + (1 - 1) log (1 - 0.6))) = (log(0.8) + log(0.2) + log(1) + log(0.6)) = -2.34$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такие расчёты можно производить для любых значений параметров, меняется только оценка вероятности $\\hat P_i$.\n",
    "\n",
    "Примечание. К сожалению, функция likelihood не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции.\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь $L(w)$, которая носит название **«функция логистических потерь»**, или **logloss**. Также часто можно встретить название **кросс-энтропия**, или **cross-entropy loss**:\n",
    "\n",
    "$L(w) = loglos = - \\sum \\limits_i^n(y_ilog(\\hat P_i) + (1-y_i)log(1 - \\hat P_i)) \\rightarrow \\min_w$\n",
    "\n",
    "$\\hat P_i = {1 \\over {1 + e^{-w_0- \\sum^m_{j=1}w_yx_j}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во избежание переобучения модели в функцию потерь логистической регрессии традиционно добавляется **регуляризация**. В реализации логистической регрессии в sklearn она немного отличается от той, что мы видели ранее для линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При L1-регуляризации мы добавляем в функцию потерь $L(w)$ штраф из суммы модулей параметров, а саму функцию logloss умножаем на коэффициент $C$:\n",
    "\n",
    "$L(w) = C \\cdot \\log loss + \\sum \\limits^m_{j=1}|w_j| \\rightarrow \\min \\limits_w$\n",
    "\n",
    "А при L2-регуляризации — штраф из суммы квадратов параметров:\n",
    "\n",
    "$L(w) = C \\cdot \\log loss + \\sum \\limits^m_{j=1}(w_j)^2 \\rightarrow \\min \\limits_w$\n",
    "\n",
    "Значение коэффициента $C$ — коэффициент, обратный коэффициенту регуляризации. Чем **больше** $C$, тем **меньше** «сила» регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предлагаем вам посмотреть на то, как будет меняться форма сигмоиды, разделяющей плоскости при минимизации функции потерь logloss (она обозначена как cross-entropy в виде концентрических кругов — вид сверху), с помощью обычного градиентного спуска (не стохастического) в виде анимации.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bfdf530906dbc60202d51e4233e10185/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_12.gif)\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/dd3061bf76c343ce85ae577b688d7be9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_13.gif)\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/a8a14407d575891d483ae85fc714243f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-2_14.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ В SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
